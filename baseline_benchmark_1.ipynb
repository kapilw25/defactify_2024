{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6EECcDt3JLY",
        "outputId": "a9ae8a21-efbc-4490-fbfc-b0b7259782ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "rnPei9hs2f63",
        "outputId": "5f11f07f-ab8a-432a-c43d-e2022cd31571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-464ff33c7722>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Set API keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TextBaseLineedited.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1AWwopP91Py_f6pwfZqvNsXWi94g5EwzF\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import Levenshtein\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set API keys\n",
        "os.environ['togetherapi'] = '30502d26c70d2d73887e3fde4f371413768a0c2f19a5024aa768c67e27022eff'\n",
        "os.environ['fireapi'] = 'fw_3ZSV7jbYCrP2D8uyXbd32z2R'\n",
        "together_apikey = os.environ.get('togetherapi')\n",
        "firework_apikey = os.environ.get('fireapi')\n",
        "\n",
        "# Define models\n",
        "together_ai_models = [\n",
        "    \"Qwen/Qwen2-72B-Instruct\",\n",
        "    # \"google/gemma-2-9b-it\",\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
        "]\n",
        "\n",
        "class Score:\n",
        "    def __init__(self, edit_score, new_text, model):\n",
        "        self.edit_score = edit_score\n",
        "        self.new_text = new_text\n",
        "        self.model = model\n",
        "\n",
        "def togetherai(question, model, api_key=together_apikey):\n",
        "    url = \"https://api.together.xyz/v1/chat/completions\"\n",
        "    formatted_prompt = f\"Regenerate provided text: TEXT = {question}\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.5,\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['choices'][0]['message']['content']\n",
        "    else:\n",
        "        print(f\"TogetherAI Error: {response.status_code}, {response.text}\")\n",
        "        return None\n",
        "\n",
        "def fireworks(question, api_key=firework_apikey, model=\"accounts/yi-01-ai/models/yi-large\"):\n",
        "    url = \"https://api.fireworks.ai/inference/v1/chat/completions\"\n",
        "    formatted_prompt = f\"Regenerate the text: TEXT={question}\\n\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.5,\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['choices'][0]['message']['content']\n",
        "    else:\n",
        "        print(f\"Fireworks Error: {response.status_code}, {response.text}\")\n",
        "        return None\n",
        "\n",
        "def get_edit_distance(text1, text2):\n",
        "    tokens1 = word_tokenize(text1)\n",
        "    tokens2 = word_tokenize(text2)\n",
        "    joined1 = \" \".join(tokens1)\n",
        "    joined2 = \" \".join(tokens2)\n",
        "    return Levenshtein.distance(joined1, joined2)\n",
        "\n",
        "def detect_text(sentence):\n",
        "    edit_distance_score = []\n",
        "    # Process using each Together AI model\n",
        "    for model in together_ai_models:\n",
        "        new_text = togetherai(sentence, model)\n",
        "        if new_text:\n",
        "            edit_score = get_edit_distance(sentence, new_text)\n",
        "            edit_distance_score.append(Score(edit_score, new_text, model))\n",
        "\n",
        "    # Process using the Fireworks model\n",
        "    new_text_fw = fireworks(sentence)\n",
        "    if new_text_fw:\n",
        "        edit_score_fw = get_edit_distance(sentence, new_text_fw)\n",
        "        edit_distance_score.append(Score(edit_score_fw, new_text_fw, \"Yi-Large\"))\n",
        "\n",
        "    return edit_distance_score\n",
        "\n",
        "# Load CSV data\n",
        "df = pd.read_csv('/content/updated_test_data 2.csv')\n",
        "texts = df[\"Text\"].astype(str).tolist()\n",
        "\n",
        "# Prepare output data\n",
        "output_data = []\n",
        "\n",
        "# Process each text and split into sentences\n",
        "for text_idx, text in enumerate(texts):\n",
        "    nltk.download('punkt_tab')\n",
        "    sentences = sent_tokenize(text)\n",
        "    for sent_idx, sentence in enumerate(sentences):\n",
        "        print(f\"\\nProcessing Sentence {text_idx+1}.{sent_idx+1}: {sentence[:50]}...\")\n",
        "        try:\n",
        "            results = detect_text(sentence)\n",
        "            if not results:\n",
        "                print(\"No valid results for this sentence.\")\n",
        "                continue\n",
        "            best_result = min(results, key=lambda x: x.edit_score)\n",
        "            output_data.append({\n",
        "                'sentence_number': f\"{text_idx+1}.{sent_idx+1}\",\n",
        "                'llm': best_result.model\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sentence {text_idx+1}.{sent_idx+1}: {e}\")\n",
        "\n",
        "# Export to CSV\n",
        "results_df = pd.DataFrame(output_data)\n",
        "results_df.to_csv('/content/drive/MyDrive/llm_detection_results.csv', index=False)\n",
        "print(\"Results saved to llm_detection_results.csv\")"
      ]
    }
  ]
}